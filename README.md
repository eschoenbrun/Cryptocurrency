# Cryptocurrency

Given the speculative nature of crytocurrencies, we set out to determine if social media metrics could accurately reflect the price fluctuations and inform a trader of a buy, sell, or hold position. Scraping data from reddit and twitter along with typical market markers, we explored different machine learning techniques to predict one day ahead pricing. 

Data Understanding and Wrangling:  
The historical data for Neo, the cryptocurrency that we will chose to focus on,was accumulated from coinmarketcap.com, starting from September 2016 through November 2017.

Data Extraction From Reddit:
We were interested to see how online conversations of a cryptocurrency correlated with the volatility of its prices.  We limited our data to Neo’s Sub-Reddit page since that seemed to be where most of the conversations about the cryptocurrency took place. 
We obtained the data through Reddit's API, PRAW, which includes post creation date, number of upvotes, number of downvotes, number of media-related files in the post, number of comments in the post, up-vote ratio and number of views for the post.

Data Extraction From Twitter:
The have been many times where the increase in a cryptocurrency's price seened to be attributed to a tweet. We figured that:
- A significant amount of 'Likes' on a tweet about the coin could make the price increase.
- An above average number of comments and likes implies the tweet has highly positive information.
- An above average number of comments and but a limited amount of likes implies that people are confused of the tweet.

Google Trends:
Since Google trends is a quantitative indicator of the amount of interest value being generated for a particular keyword or search term, it is a great way to track the growth of active Neo users. It serves as a proxy for the engagement of active buyers of the crypto currency as they check information, news, articles of interest and price of the cryptocurrency. We used pytrends module to extract the engagement interest levels in NEO in the time period we were considering to train our model.

Modeling 
Sequential data is a specific type of prediction that typical machine learning algorithms must be modified to implement.  This is because the core structure of the data relies on the order that the data is collected.  Standard time series forecasting techniques use a combination of lag variables (AutoRegression) and moving average of error terms (Moving Average) to calculate the impact that previous periods have on the forecasted values - a common forecasting tool known as ARIMA.  However, there are few machine learning techniques, Markov Model Monte Carlo (MCMC) being the notable exception, that retain the sequential nature of the data.  In order to explore, then, the implementation of machine learning algorithms to predict the cryptocurrency value, we needed to find a way to reconstruct that ordinal structure.  
Vector Autoregression (VAR) is a particular type of ARIMA which takes exogeneous data along with the target variable, turning the matrix into multiple time series.  Because our data contained Reddit data, Twitter data, and the like, we had to set up the matrix where we not only accounted for the lags of the target variable, but the lags of all the variables that would be a part of the model.
Our first task was to prepare the data for modeling. Firstly, the range of the data between its origin and current values was widely disparate.  That means that any imputation that we would want to do would be inaccurate and forecasting accuracy would be faulty.  In order to place everything on the same scale, we took the log of all the values in each vector to normalize the data.  Secondly, for forecasting models to properly function, the data must be stationary, where the oscillations of peaks and troughs revolve around the mean.  Our data had an upwards trend, and so we needed to difference the data using Box-Cox Transformation.
Using the ‘Pyramid’ package, a Python wrapper to Rob Hyndman’s ‘Forecast’ package in R, we calculated the model order for the appropriate amount of lag periods that correlate the most with our target.  This function searches for the optimal parameters for the ARIMA model using uses a stepwise algorithm laid out in a paper by Hyndman and Khandakar (2008), and we fit a random search to it that is much faster than an exhaustive one.  We incorporated this function in a loop through every vector, so that if the lags differed for any of the variables, we could use different amount of lagged variables.
Once the optimal amount of lag variables was calculated, we created a new column for each lag period for that variable.  In this case, each variable had the same lag value of five, so each variable would have six columns for each observation; the current value of the variable and the previous five periods’ values.  The final result was a matrix of 72 columns which accounted for the variables and each of their lagged periods.  We removed the dates at the end of the dataframe which had null values due to creating the shift since the dates were months prior and did not seem to affect current prices.  We also shifted the target variable to the t-1 observation to reflect the fact that we are using the values of the previous day to predict the next day’s Neo price.  Namely, if the last observation was 12/08/2017 and we had a complete data for that observation, we would be trying to predict the close price for 12/09/2017.  As the observations were initially set up that the close price represented the same date as all the other variables, i.e. 12/08/2017, we needed to shift the close column down so that the observations on 12/07/2017 would align with the close value of 12/08/2017 and the variables on  12/08/2017 would predict the close price of  12/09/2017.

Dimension Reduction
Because of the ‘wider’ format of the data (slightly over 400 rows and 72 columns), we wanted to see if reducing the dimensionality would shrink the dataset while retaining the forecasting power the data held.  The first model we tried in order to reduce the complexity of the model was Principal Component Analysis (PCA).  With sklearn’s decomposition function we ran the Principal Component Analysis and found that the top five principal components accounted for over 92% of the total variance.  When we applied this to a simple linear regression, the accuracy of the model suffered significantly resulting in a R2 of .57.  We chose to run these variables with a SVM and ADABoost model, which will be discussed below.

Feature Importance
Our second method to reduce the size of the dataset was using Random Forest’s Feature Importance capabilities.  After fitting the model, we found the following results:

There are several variables that have a significant improvement in importance than others, however the progression of decrease in importance was shallow and therefore we needed to test what our threshold should be and which features to choose. We built a loop in Python that started with the lowest threshold and progressively increased the limit while calculating the accuracy of each of our machine learning models at each threshold, and concluded from both that the 10 highest features would give us the best accuracy.
Manual Removal of Correlated Variables:
One method we tried as a benchmark was to perform a linear regression on all the variables and comparing it after manually removing the variables whose P-Value and the β value imply that they do not belong. We were able to achieve a R-squared of 0.97 with 48 variables, which was an improvement, and most of the variables made sense to us as some of our hypotheses was proven that the correlated variables reduced accuracy.

Time Series Cross Validation
As mentioned, sequential data does not adhere to the more common rules of machine learning tactics given it sequential structure.  This is particularly true with regard to cross validation, where the shuffling of randomized observations could result in predicting on past values given future data, and undermine the accuracy of its forecasting abilities.  Instead, the sliding window, or rolling origin, method is suggested.  This process takes a portion of the data and fits the model and predicts against the next X values.  It then increases the training size, using the same original data and adding on an additional segment - encompassing the validation component.  To illustrate, if the initial training data ranged from 10/01/2016 to 12/01/2016 while the validation set would be from 12/01/2016 to 01/01/2017 , the following model would train on 10/01/2016 to 01/01/2017 and validate on 02/01/2017.  This retains the sequential nature of the data while still being able to perform a kfold-like process.  

Support Vector Machine
We chose a support vector machine based on research by Stefan Ruping from the University of Dortmund, who compared multiple kernels of SVM regressions and concluded that SVM performed well when it used a RBF kernel.

ADABoost
Ensemble methods often increase an individual models accuracy given its ability to reduce overfitting and decrease variance by averaging out individual  models that may have overfit.  Given the number of variables, we were curious if a boosting method would counteract any overfitting the number of variables would produce.
Process

Once we reduced the amount of variables in the dataframe, either by PCA or Random Forest, we built a loop to implement the rolling origin time series validation technique with each of our machine learning models and average the mean absolute percentage errors (MAPE) to come out with a final error metric to analyse the performance of the model.  We then tested the fitted model on our final held-out test data to ascertain the final performance of the model.  To compare, we also ran the same sequence on the original 72 variables as a benchmark for the performance of the dimension reduction/feature importance implementations.


Findings
It seems as if each of the techniques are underfitting the data, which is very curious.  We hypothesize that this is because of the volatile nature of the data and the models not accurately picking up on the correct model.  Because it is underfitting, we would suggest that a more complex algorithm like Neural Networks that can fit the model with more nuance.
Facebook has recently released a forecasting package called ‘Prophet’ that is predicated on the Markov Chain Monte Carlo method.  While we did want to try it out, at the moment it only accepts the target variable as its input.  We would have liked to use it nonetheless to give us an idea of the implementation of a MCMC model, especially because of the sequential aspect that the MCMC model retains, but even the computers in the Hanlon lab took an extraneous amount of time processing and we were not able to gain any significant results. 
